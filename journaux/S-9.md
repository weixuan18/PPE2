#  Séance 9

## 6 avril 2023
**ZHANG**: Suite à la séance d'hier, la structure des données à renvoyer se montre plus éclaircie pour nous. Je me vois obligé de remodifier notre script **extraire_deux.py** dans la branche **main** afin que le résultat *xml* devienne plus conforme à la plulart de ce à quoi le professeur s'attend. Après son exécution, voici la forme du fichier sorti:
```
<Corpus begin="2022-01-01" end="2023-01-01" categories="international;une">
<article date="2022-03-14">
<title>
Dans l’est de la RDC, les ADF, un ennemi insaisissable pour les populations et pour l’armée
</title>
<desc>
Après vingt-sept ans d’exactions, les motivations du groupe armé créé en Ouganda, rallié à l’Etat islamique en 2019, restent floues.
</desc>
</article>
```
Le prochain serait alors de travailler avec le module de stockage des données `dataclasses` permettant ainsi de générer un résultat au format *xml* aussi bien que *json*  , et ensuite de tester chacun un outil de corpus pour effecuter des annotations morpho-syntaxiques liées aux titres et descriptions des articles. 

J'ai fini de redonner un script python **annotation_spacy.py** ainsi qu'un résultat intitulé **sortie.xml** dans ma branche **zxh-s9** nouvellement créée. Le contenu sorti est assez satisfaisant avec les tokenisations dedans. Le problème pour l'instant, c'est justement que le script est vraiment lourd qui nécéssaite de se réorganiser à l'aide des plusieures fonctions possibles. Je décide de le faire après. 
***

## 8 avril 2023
**HUANG** : Après une séance consacrée à la correction du script pour les différentes sorties et à une courte introduction des analyseurs syntaxiques, il me reste quelques tâches à compléter avant d'entamer les analyses syntaxiques.

Tout d'abord, en basant sur les scripts du prof (extraction et datastructures), j'ai fait des ajouts au début des scripts sortie (export_xml.py et export_json.py) pour que la catégorie entrée puisse également être affiché dans les fichiers sorties (`json` et `xml`). L'affichage de `pickle` nous paraît difficile à analyser par humain, et nous l'avons donc abandonné comme sortie.

Après ces prétraitements de format, il est maintenant possible de procéder à l'analyse syntaxique. J'ai choisi cette fois l'analyseur automatique *Stanza*, ce que j'ai testé un peu en ligne avec son demo.

La structure et la fonction de *Stanza* ressemblent beaucoup à celles de *Spacy*, qui demande tous dmabord la tokenization, puis les autres fonctions comme `lemma` et `upos`. Il est à noter que la grande différence au début, c'est que *Stanza* sépare le corpus tout d'abord en phrases, puis en tokens (words). Les fonctions `text, lemma et upos` coresspondent au contenu attendu : `forme, lemme et pos`.

Par rapport à mon camarade, les lignes de programmes sont ajoutées dans les scripts de sortie (`export_json.py` et `export_xml.py`). Puisque maintenant il m'est encore confus de chosir un des deux formats, j'ai fait respectivement des modifications dans les deux scripts, afin que la forme de sortie soit idéale et corresponde à celle de mon camarade :

    Pour la sortie xml :

    <corpus begin="2022-01-01" end="2022-01-02" categorie="sport">
        <content>
        <article>
            <title>
            Coupe de France de football : Bastia et Nancy créent la surprise
            </title>
            <description>
            Les deux clubs de Ligue 2 ont écarté respectivement Clermont et Rennes, deux clubs de Ligue 1. Brest et Montpellier ont dominé Bordeaux et Strasbourg. Nantes est également qualifié.
            </description>
            <categorie/>
            <analyse>
                <token forme="Coupe" lemma="coupe" pos="NOUN"/>
                ...
            </analyse>


    Pour la sortie json :

    {
    "categories": [
        "sport"
    ],
    "begin": "2022-01-01",
    "end": "2022-01-02",
    "chemin": "./Corpus/2022",
    "articles": [
        {
            "titre": "Coupe de France de football\u00a0: Bastia et Nancy cr\u00e9ent la surprise",
            "description": "Les deux clubs de Ligue 2 ont \u00e9cart\u00e9 respectivement Clermont et Rennes, deux clubs de Ligue 1. Brest et Montpellier ont domin\u00e9 Bordeaux et Strasbourg. Nantes est \u00e9galement qualifi\u00e9.",
            "analyse": {
                "tokens": [
                    {
                        "forme": "Coupe",
                        "lemma": "coupe",
                        "pos": "NOUN"
                    },
        ...
                ]
            }
        ]
    }


Les scripts complets se trouvent dans la branche **hyd-s9** (dont le contenu est les nouvelles de la catégorie *sport* en 2022-01-01), les noms de ces fichiers sont `corpus.json`et `corpus.xml`.

Mais le script ne peut que prendre une catégorie en tant que source entrée, ce qui demanderait une amélioration pour faciliter l'analyse multidimensionelle ultérieure.

***
## 11 avril 2023
**ZHANG**: Après avoir réussi à la tokenisation avec le module `spaCy` ainsi que le remplissage dans le xml fichier, j'ai ensuite réorganisé dans ma branche **zxh-s9** presque toutes fonctions utilisées, y compris `datastructures`, `xml_export` ,`json_export`, dans un seul script renommé **extraire_tout_spacy.py**. 

Par rapport à la remise en question des *traitement multi-catégorielle* emanée par mon camarade, j'envisage de le résolver avec le code suivant:
```
    if len(corpus.categories) == 1:
        root.attrib['categories'] = corpus.categories[0]
    else:
        root.attrib['categories'] = ",".join(corpus.categories)
```
C'est-à-dire que je vais stoker tous enssemble les catégories plusieures possibles dans l'attribut de la racine du xml.

On veut vraisemblablement générer de différents formats du document comme résultat. tels que `xml` `json` ou bien `stdout`. Compte tenue de ce fait, j'ai donc amélioré la fonction **main** à l'aide des plusieures boucles `if` dans ma script. Voici le code:
```
        if args.o is None:
            for title, description in extraire_td(file.as_posix()):
                print(title, description)
        if args.o is not None:
            if args.o.endswith(".xml"):
                write_xml(file, args.o)
            elif args.o.endswith(".json"):
                write_json(file, args.o)
            else:
                print("Format de fichier non supporté")
```
***
**HUANG** : Merci à mon camarade pour la proposition de la solution concercant *multi-catégories* ! Il est maintenant possible d'insérer les codes proposés ci-dessus dans mon script pour résoudre le problème.

Concernant l'analyseur syntaxique, après avoir testé le code de `Spacy` de mon camarade (dans la branche **zxh-s9**), il me semble que `Spacy` marche plus efficacement que `Stanza`, car le premier utilise son bagage de source déjà installé, alors que le dernier doit rappeler chaque tour de code `download fr` pour une analyse, et même si c'est pour une catégorie d'un seul jour, ça prend assez du temps. 

Ainsi, il faut peut-être voir la performance de `Trankit` pour finalement déterminer l'analyseur le plus pertinent pour notre travail de grande quantité.

***
**CHENG** :

<span style="color:black; background-color:beige;">Tâche</span>: 
- Créer des scripts en définissant la structure d'xml `datastructure.py` et `export_xml`
- Utiliser l'outil de traitement du langage -- `Trankit` (qui sert à analyser les textes, telle que la segmentation, l'étiquettage **POS**)

**`Module Trankit`**:  

inconvénients : 

- Très lourd. C'est un module volumineux, il prend du temps pour le télécharger. 


Avantage : 
- Un bon outil d'analyseur. Il utilise des modèles pré-entraînés de haute qualité pour effectuer plusieurs tâches de traitement du langage naturel telles que l'étiquetage de parties du discours, l'analyse de dépendances syntaxiques, la lemmatisation, etc.

Résumé pour S9 : 

J'ai écrit une première version de script `annotation_trankit` en utilisant le module trankit pour annoter (analyser) les `forms`, les `lemmas` ainsi que leurs `POS`. Je tente de le corriger après l'explication du prof.

(Mais j'étais un peu triste et déçue cette semaine car je n'arrivais toujours pas à bien utiliser le module `trankit` et je ne pouvais même pas résoudre le problème rencontré (***blocage** après l'activation de la langue française*). Je n'ai pas pu utiliser les modules entraînés pour le français, même s'il semble que la logique soit correcte, je ne peux pas vérifier si mon script marche bien)

Je suis perdue :(

```




